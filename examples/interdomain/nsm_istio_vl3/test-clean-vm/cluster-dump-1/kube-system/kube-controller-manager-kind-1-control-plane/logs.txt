==== START logs for container kube-controller-manager of pod kube-system/kube-controller-manager-kind-1-control-plane ====
I1205 06:49:16.632230       1 serving.go:348] Generated self-signed cert in-memory
I1205 06:49:17.055429       1 controllermanager.go:178] Version: v1.25.3
I1205 06:49:17.055484       1 controllermanager.go:180] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1205 06:49:17.056379       1 secure_serving.go:210] Serving securely on 127.0.0.1:10257
I1205 06:49:17.056388       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1205 06:49:17.056627       1 leaderelection.go:248] attempting to acquire leader lease kube-system/kube-controller-manager...
I1205 06:49:17.056701       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1205 06:49:17.056648       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
E1205 06:49:21.473532       1 leaderelection.go:330] error retrieving resource lock kube-system/kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
I1205 06:49:24.299983       1 leaderelection.go:258] successfully acquired lease kube-system/kube-controller-manager
I1205 06:49:24.300249       1 event.go:294] "Event occurred" object="kube-system/kube-controller-manager" fieldPath="" kind="Lease" apiVersion="coordination.k8s.io/v1" type="Normal" reason="LeaderElection" message="kind-1-control-plane_7a0c5498-9dac-4080-bd49-a2ee47c398f0 became leader"
I1205 06:49:24.312126       1 shared_informer.go:255] Waiting for caches to sync for tokens
I1205 06:49:24.319572       1 controllermanager.go:603] Started "podgc"
W1205 06:49:24.319597       1 core.go:232] configure-cloud-routes is set, but no cloud provider specified. Will not configure cloud provider routes.
W1205 06:49:24.319602       1 controllermanager.go:581] Skipping "route"
I1205 06:49:24.319826       1 gc_controller.go:99] Starting GC controller
I1205 06:49:24.319839       1 shared_informer.go:255] Waiting for caches to sync for GC
I1205 06:49:24.344162       1 controllermanager.go:603] Started "clusterrole-aggregation"
I1205 06:49:24.344439       1 clusterroleaggregation_controller.go:194] Starting ClusterRoleAggregator
I1205 06:49:24.344507       1 shared_informer.go:255] Waiting for caches to sync for ClusterRoleAggregator
I1205 06:49:24.352368       1 controllermanager.go:603] Started "cronjob"
I1205 06:49:24.352505       1 cronjob_controllerv2.go:135] "Starting cronjob controller v2"
I1205 06:49:24.352515       1 shared_informer.go:255] Waiting for caches to sync for cronjob
I1205 06:49:24.360133       1 controllermanager.go:603] Started "bootstrapsigner"
I1205 06:49:24.360164       1 shared_informer.go:255] Waiting for caches to sync for bootstrap_signer
I1205 06:49:24.366576       1 controllermanager.go:603] Started "csrapproving"
I1205 06:49:24.366741       1 certificate_controller.go:112] Starting certificate controller "csrapproving"
I1205 06:49:24.366748       1 shared_informer.go:255] Waiting for caches to sync for certificate-csrapproving
I1205 06:49:24.372248       1 controllermanager.go:603] Started "tokencleaner"
I1205 06:49:24.372357       1 tokencleaner.go:118] Starting token cleaner controller
I1205 06:49:24.372367       1 shared_informer.go:255] Waiting for caches to sync for token_cleaner
I1205 06:49:24.372375       1 shared_informer.go:262] Caches are synced for token_cleaner
I1205 06:49:24.379776       1 node_ipam_controller.go:91] Sending events to api server.
I1205 06:49:24.413022       1 shared_informer.go:262] Caches are synced for tokens
I1205 06:49:34.398726       1 range_allocator.go:76] Sending events to api server.
I1205 06:49:34.398901       1 range_allocator.go:110] No Secondary Service CIDR provided. Skipping filtering out secondary service addresses.
I1205 06:49:34.398921       1 controllermanager.go:603] Started "nodeipam"
I1205 06:49:34.398965       1 node_ipam_controller.go:154] Starting ipam controller
I1205 06:49:34.398975       1 shared_informer.go:255] Waiting for caches to sync for node
I1205 06:49:34.406948       1 controllermanager.go:603] Started "endpoint"
I1205 06:49:34.406985       1 endpoints_controller.go:182] Starting endpoint controller
I1205 06:49:34.407075       1 shared_informer.go:255] Waiting for caches to sync for endpoint
I1205 06:49:34.413480       1 controllermanager.go:603] Started "endpointslicemirroring"
I1205 06:49:34.413622       1 endpointslicemirroring_controller.go:216] Starting EndpointSliceMirroring controller
I1205 06:49:34.413654       1 shared_informer.go:255] Waiting for caches to sync for endpoint_slice_mirroring
I1205 06:49:34.419803       1 controllermanager.go:603] Started "deployment"
I1205 06:49:34.419983       1 deployment_controller.go:160] "Starting controller" controller="deployment"
I1205 06:49:34.420021       1 shared_informer.go:255] Waiting for caches to sync for deployment
I1205 06:49:34.426141       1 controllermanager.go:603] Started "attachdetach"
I1205 06:49:34.426221       1 attach_detach_controller.go:328] Starting attach detach controller
I1205 06:49:34.426231       1 shared_informer.go:255] Waiting for caches to sync for attach detach
I1205 06:49:34.432670       1 controllermanager.go:603] Started "persistentvolume-expander"
I1205 06:49:34.432837       1 expand_controller.go:340] Starting expand controller
I1205 06:49:34.433065       1 shared_informer.go:255] Waiting for caches to sync for expand
I1205 06:49:34.441412       1 controllermanager.go:603] Started "pvc-protection"
I1205 06:49:34.441715       1 pvc_protection_controller.go:103] "Starting PVC protection controller"
I1205 06:49:34.441753       1 shared_informer.go:255] Waiting for caches to sync for PVC protection
I1205 06:49:34.448187       1 controllermanager.go:603] Started "ephemeral-volume"
I1205 06:49:34.448638       1 controller.go:169] Starting ephemeral volume controller
I1205 06:49:34.448713       1 shared_informer.go:255] Waiting for caches to sync for ephemeral
I1205 06:49:34.457468       1 controllermanager.go:603] Started "replicationcontroller"
I1205 06:49:34.457814       1 replica_set.go:205] Starting replicationcontroller controller
I1205 06:49:34.457863       1 shared_informer.go:255] Waiting for caches to sync for ReplicationController
I1205 06:49:34.465502       1 controllermanager.go:603] Started "statefulset"
I1205 06:49:34.465662       1 stateful_set.go:152] Starting stateful set controller
I1205 06:49:34.465734       1 shared_informer.go:255] Waiting for caches to sync for stateful set
E1205 06:49:34.486006       1 core.go:92] Failed to start service controller: WARNING: no cloud provider provided, services of type LoadBalancer will fail
W1205 06:49:34.486050       1 controllermanager.go:581] Skipping "service"
E1205 06:49:34.537979       1 core.go:218] failed to start cloud node lifecycle controller: no cloud provider provided
W1205 06:49:34.538039       1 controllermanager.go:581] Skipping "cloud-node-lifecycle"
I1205 06:49:34.687119       1 controllermanager.go:603] Started "ttl-after-finished"
I1205 06:49:34.687214       1 ttlafterfinished_controller.go:109] Starting TTL after finished controller
I1205 06:49:34.687223       1 shared_informer.go:255] Waiting for caches to sync for TTL after finished
I1205 06:49:34.935068       1 garbagecollector.go:154] Starting garbage collector controller
I1205 06:49:34.935420       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I1205 06:49:34.935092       1 controllermanager.go:603] Started "garbagecollector"
I1205 06:49:34.935590       1 graph_builder.go:291] GraphBuilder running
I1205 06:49:35.234836       1 controllermanager.go:603] Started "disruption"
I1205 06:49:35.234854       1 disruption.go:421] Sending events to api server.
I1205 06:49:35.235240       1 disruption.go:432] Starting disruption controller
I1205 06:49:35.235248       1 shared_informer.go:255] Waiting for caches to sync for disruption
I1205 06:49:35.286627       1 node_lifecycle_controller.go:497] Controller will reconcile labels.
I1205 06:49:35.286731       1 controllermanager.go:603] Started "nodelifecycle"
I1205 06:49:35.286806       1 node_lifecycle_controller.go:532] Sending events to api server.
I1205 06:49:35.286824       1 node_lifecycle_controller.go:543] Starting node controller
I1205 06:49:35.286833       1 shared_informer.go:255] Waiting for caches to sync for taint
I1205 06:49:35.436825       1 controllermanager.go:603] Started "persistentvolume-binder"
I1205 06:49:35.437058       1 pv_controller_base.go:318] Starting persistent volume controller
I1205 06:49:35.437070       1 shared_informer.go:255] Waiting for caches to sync for persistent volume
I1205 06:49:35.587446       1 controllermanager.go:603] Started "endpointslice"
I1205 06:49:35.587620       1 endpointslice_controller.go:261] Starting endpoint slice controller
I1205 06:49:35.587645       1 shared_informer.go:255] Waiting for caches to sync for endpoint_slice
I1205 06:49:35.901052       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for horizontalpodautoscalers.autoscaling
I1205 06:49:35.901149       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for rolebindings.rbac.authorization.k8s.io
I1205 06:49:35.901179       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for networkpolicies.networking.k8s.io
I1205 06:49:35.901190       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for roles.rbac.authorization.k8s.io
I1205 06:49:35.901207       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for statefulsets.apps
I1205 06:49:35.901218       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for ingresses.networking.k8s.io
I1205 06:49:35.901228       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for poddisruptionbudgets.policy
I1205 06:49:35.901237       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for leases.coordination.k8s.io
I1205 06:49:35.901247       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for daemonsets.apps
I1205 06:49:35.901291       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for replicasets.apps
I1205 06:49:35.901349       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for csistoragecapacities.storage.k8s.io
I1205 06:49:35.901411       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for limitranges
I1205 06:49:35.901462       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for endpoints
I1205 06:49:35.901477       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for deployments.apps
I1205 06:49:35.901487       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for jobs.batch
I1205 06:49:35.901503       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for endpointslices.discovery.k8s.io
W1205 06:49:35.901533       1 shared_informer.go:533] resyncPeriod 20h34m46.538545048s is smaller than resyncCheckPeriod 21h46m57.136546574s and the informer has already started. Changing it to 21h46m57.136546574s
I1205 06:49:35.901617       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for serviceaccounts
I1205 06:49:35.901734       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for podtemplates
I1205 06:49:35.901866       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for cronjobs.batch
W1205 06:49:35.901886       1 shared_informer.go:533] resyncPeriod 21h23m19.908408639s is smaller than resyncCheckPeriod 21h46m57.136546574s and the informer has already started. Changing it to 21h46m57.136546574s
I1205 06:49:35.901944       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for controllerrevisions.apps
I1205 06:49:35.901974       1 controllermanager.go:603] Started "resourcequota"
I1205 06:49:35.902686       1 resource_quota_controller.go:277] Starting resource quota controller
I1205 06:49:35.902753       1 shared_informer.go:255] Waiting for caches to sync for resource quota
I1205 06:49:35.902780       1 resource_quota_monitor.go:295] QuotaMonitor running
I1205 06:49:36.035741       1 controllermanager.go:603] Started "serviceaccount"
I1205 06:49:36.035758       1 serviceaccounts_controller.go:117] Starting service account controller
I1205 06:49:36.035880       1 shared_informer.go:255] Waiting for caches to sync for service account
I1205 06:49:36.336947       1 controllermanager.go:603] Started "horizontalpodautoscaling"
I1205 06:49:36.337058       1 horizontal.go:168] Starting HPA controller
I1205 06:49:36.337069       1 shared_informer.go:255] Waiting for caches to sync for HPA
I1205 06:49:36.384883       1 controllermanager.go:603] Started "csrcleaner"
I1205 06:49:36.384955       1 cleaner.go:82] Starting CSR cleaner controller
I1205 06:49:36.536925       1 controllermanager.go:603] Started "ttl"
I1205 06:49:36.536948       1 ttl_controller.go:120] Starting TTL controller
I1205 06:49:36.537033       1 shared_informer.go:255] Waiting for caches to sync for TTL
I1205 06:49:36.798573       1 controllermanager.go:603] Started "namespace"
I1205 06:49:36.798885       1 namespace_controller.go:200] Starting namespace controller
I1205 06:49:36.798962       1 shared_informer.go:255] Waiting for caches to sync for namespace
I1205 06:49:36.939209       1 controllermanager.go:603] Started "daemonset"
I1205 06:49:36.939996       1 daemon_controller.go:291] Starting daemon sets controller
I1205 06:49:36.940076       1 shared_informer.go:255] Waiting for caches to sync for daemon sets
I1205 06:49:37.086965       1 controllermanager.go:603] Started "replicaset"
I1205 06:49:37.087099       1 replica_set.go:205] Starting replicaset controller
I1205 06:49:37.087116       1 shared_informer.go:255] Waiting for caches to sync for ReplicaSet
I1205 06:49:37.237595       1 controllermanager.go:603] Started "root-ca-cert-publisher"
I1205 06:49:37.237731       1 publisher.go:107] Starting root CA certificate configmap publisher
I1205 06:49:37.237748       1 shared_informer.go:255] Waiting for caches to sync for crt configmap
I1205 06:49:37.389608       1 controllermanager.go:603] Started "job"
I1205 06:49:37.389764       1 job_controller.go:196] Starting job controller
I1205 06:49:37.389775       1 shared_informer.go:255] Waiting for caches to sync for job
I1205 06:49:37.437042       1 certificate_controller.go:112] Starting certificate controller "csrsigning-kubelet-serving"
I1205 06:49:37.437130       1 shared_informer.go:255] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I1205 06:49:37.437212       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I1205 06:49:37.437852       1 certificate_controller.go:112] Starting certificate controller "csrsigning-kubelet-client"
I1205 06:49:37.437935       1 shared_informer.go:255] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I1205 06:49:37.437959       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I1205 06:49:37.438232       1 certificate_controller.go:112] Starting certificate controller "csrsigning-kube-apiserver-client"
I1205 06:49:37.438373       1 shared_informer.go:255] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I1205 06:49:37.438401       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I1205 06:49:37.438262       1 controllermanager.go:603] Started "csrsigning"
I1205 06:49:37.438281       1 certificate_controller.go:112] Starting certificate controller "csrsigning-legacy-unknown"
I1205 06:49:37.438685       1 shared_informer.go:255] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I1205 06:49:37.438293       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I1205 06:49:37.586260       1 controllermanager.go:603] Started "pv-protection"
I1205 06:49:37.586301       1 pv_protection_controller.go:79] Starting PV protection controller
I1205 06:49:37.586623       1 shared_informer.go:255] Waiting for caches to sync for PV protection
I1205 06:49:37.589598       1 shared_informer.go:255] Waiting for caches to sync for resource quota
W1205 06:49:37.599455       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="kind-1-control-plane" does not exist
I1205 06:49:37.600201       1 shared_informer.go:262] Caches are synced for node
I1205 06:49:37.600379       1 range_allocator.go:166] Starting range CIDR allocator
I1205 06:49:37.600435       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I1205 06:49:37.600456       1 shared_informer.go:262] Caches are synced for cidrallocator
I1205 06:49:37.607823       1 range_allocator.go:367] Set node kind-1-control-plane PodCIDR to [10.244.0.0/24]
I1205 06:49:37.609748       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I1205 06:49:37.620582       1 shared_informer.go:262] Caches are synced for deployment
I1205 06:49:37.620706       1 shared_informer.go:262] Caches are synced for GC
I1205 06:49:37.627298       1 shared_informer.go:262] Caches are synced for attach detach
I1205 06:49:37.633444       1 shared_informer.go:262] Caches are synced for expand
I1205 06:49:37.635485       1 shared_informer.go:262] Caches are synced for disruption
I1205 06:49:37.637829       1 shared_informer.go:262] Caches are synced for crt configmap
I1205 06:49:37.637888       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I1205 06:49:37.637912       1 shared_informer.go:262] Caches are synced for persistent volume
I1205 06:49:37.637945       1 shared_informer.go:262] Caches are synced for TTL
I1205 06:49:37.639028       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I1205 06:49:37.637957       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I1205 06:49:37.639786       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1205 06:49:37.640600       1 shared_informer.go:262] Caches are synced for daemon sets
I1205 06:49:37.642612       1 shared_informer.go:262] Caches are synced for PVC protection
I1205 06:49:37.645072       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I1205 06:49:37.649039       1 shared_informer.go:262] Caches are synced for ephemeral
I1205 06:49:37.658024       1 shared_informer.go:262] Caches are synced for ReplicationController
I1205 06:49:37.660638       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I1205 06:49:37.665987       1 shared_informer.go:262] Caches are synced for stateful set
I1205 06:49:37.667312       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I1205 06:49:37.686763       1 shared_informer.go:262] Caches are synced for PV protection
I1205 06:49:37.686953       1 shared_informer.go:262] Caches are synced for taint
I1205 06:49:37.687081       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
W1205 06:49:37.687431       1 node_lifecycle_controller.go:1058] Missing timestamp for Node kind-1-control-plane. Assuming now as a timestamp.
I1205 06:49:37.687653       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I1205 06:49:37.687796       1 shared_informer.go:262] Caches are synced for ReplicaSet
I1205 06:49:37.687959       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
I1205 06:49:37.688118       1 taint_manager.go:209] "Sending events to api server"
I1205 06:49:37.689851       1 shared_informer.go:262] Caches are synced for job
I1205 06:49:37.690418       1 event.go:294] "Event occurred" object="kind-1-control-plane" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node kind-1-control-plane event: Registered Node kind-1-control-plane in Controller"
I1205 06:49:37.690736       1 shared_informer.go:262] Caches are synced for TTL after finished
I1205 06:49:37.704554       1 event.go:294] "Event occurred" object="kube-system/kube-controller-manager-kind-1-control-plane" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1205 06:49:37.704872       1 event.go:294] "Event occurred" object="local-path-storage/local-path-provisioner" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set local-path-provisioner-684f458cdd to 1"
I1205 06:49:37.710077       1 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-565d847f94 to 2"
I1205 06:49:37.710168       1 event.go:294] "Event occurred" object="kube-system/etcd-kind-1-control-plane" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1205 06:49:37.710218       1 event.go:294] "Event occurred" object="kube-system/kube-scheduler-kind-1-control-plane" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1205 06:49:37.714836       1 event.go:294] "Event occurred" object="kube-system/kube-apiserver-kind-1-control-plane" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I1205 06:49:37.736433       1 shared_informer.go:262] Caches are synced for service account
I1205 06:49:37.737748       1 shared_informer.go:262] Caches are synced for HPA
I1205 06:49:37.787875       1 shared_informer.go:262] Caches are synced for endpoint_slice
I1205 06:49:37.799315       1 shared_informer.go:262] Caches are synced for namespace
I1205 06:49:37.802993       1 shared_informer.go:262] Caches are synced for resource quota
I1205 06:49:37.807423       1 shared_informer.go:262] Caches are synced for endpoint
I1205 06:49:37.814131       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I1205 06:49:37.853099       1 shared_informer.go:262] Caches are synced for cronjob
I1205 06:49:37.890563       1 shared_informer.go:262] Caches are synced for resource quota
I1205 06:49:38.212087       1 shared_informer.go:262] Caches are synced for garbage collector
I1205 06:49:38.235711       1 shared_informer.go:262] Caches are synced for garbage collector
I1205 06:49:38.236146       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1205 06:49:38.298310       1 event.go:294] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-qhvqb"
I1205 06:49:38.300611       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-s7mnl"
I1205 06:49:38.394946       1 event.go:294] "Event occurred" object="local-path-storage/local-path-provisioner-684f458cdd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: local-path-provisioner-684f458cdd-lk6jk"
I1205 06:49:38.395123       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-565d847f94-tmmbq"
I1205 06:49:38.410067       1 event.go:294] "Event occurred" object="kube-system/coredns-565d847f94" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-565d847f94-xrfj2"
W1205 06:49:42.870924       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="kind-1-worker2" does not exist
I1205 06:49:42.880364       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-qs659"
I1205 06:49:42.884062       1 event.go:294] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-g6km2"
I1205 06:49:42.892627       1 range_allocator.go:367] Set node kind-1-worker2 PodCIDR to [10.244.1.0/24]
W1205 06:49:43.100262       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="kind-1-worker" does not exist
I1205 06:49:43.109572       1 range_allocator.go:367] Set node kind-1-worker PodCIDR to [10.244.2.0/24]
I1205 06:49:43.115055       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-6kgx4"
I1205 06:49:43.122459       1 event.go:294] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-hckbr"
W1205 06:49:47.689587       1 node_lifecycle_controller.go:1058] Missing timestamp for Node kind-1-worker2. Assuming now as a timestamp.
W1205 06:49:47.689692       1 node_lifecycle_controller.go:1058] Missing timestamp for Node kind-1-worker. Assuming now as a timestamp.
I1205 06:49:47.690420       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I1205 06:49:47.690386       1 event.go:294] "Event occurred" object="kind-1-worker2" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node kind-1-worker2 event: Registered Node kind-1-worker2 in Controller"
I1205 06:49:47.690511       1 event.go:294] "Event occurred" object="kind-1-worker" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node kind-1-worker event: Registered Node kind-1-worker in Controller"
W1205 06:49:53.270266       1 topologycache.go:199] Can't get CPU or zone information for kind-1-worker2 node
W1205 06:49:53.428356       1 topologycache.go:199] Can't get CPU or zone information for kind-1-worker node
I1205 06:50:14.800677       1 event.go:294] "Event occurred" object="metallb-system/speaker" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: speaker-6tg6t"
I1205 06:50:14.805055       1 event.go:294] "Event occurred" object="metallb-system/controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set controller-6658b8446c to 1"
I1205 06:50:14.814396       1 event.go:294] "Event occurred" object="metallb-system/controller-6658b8446c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: controller-6658b8446c-hn4sg"
I1205 06:50:14.815125       1 event.go:294] "Event occurred" object="metallb-system/speaker" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: speaker-7sd6z"
I1205 06:51:37.299526       1 event.go:294] "Event occurred" object="exposed-kube-dns" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToCreateEndpoint" message="Failed to create endpoint for service kube-system/exposed-kube-dns: endpoints \"exposed-kube-dns\" already exists"
I1205 06:51:44.552567       1 event.go:294] "Event occurred" object="spire/spire-server" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod spire-server-0 in StatefulSet spire-server successful"
I1205 06:51:44.569328       1 event.go:294] "Event occurred" object="spire/spire-agent" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: spire-agent-4mvzx"
I1205 06:51:44.611095       1 event.go:294] "Event occurred" object="spire/spire-agent" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: spire-agent-4w5jm"
I1205 06:52:07.939720       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for spiffeids.spiffeid.spiffe.io
I1205 06:52:07.939805       1 shared_informer.go:255] Waiting for caches to sync for resource quota
I1205 06:52:08.040831       1 shared_informer.go:262] Caches are synced for resource quota
I1205 06:52:08.268902       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I1205 06:52:08.268989       1 shared_informer.go:262] Caches are synced for garbage collector
I1205 06:52:39.484322       1 event.go:294] "Event occurred" object="nsm-system/admission-webhook-k8s" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set admission-webhook-k8s-6664486479 to 1"
I1205 06:52:39.499916       1 event.go:294] "Event occurred" object="nsm-system/admission-webhook-k8s-6664486479" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: admission-webhook-k8s-6664486479-nx9n6"
I1205 06:52:39.520586       1 event.go:294] "Event occurred" object="nsm-system/nsmgr-proxy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set nsmgr-proxy-f6f56b957 to 1"
I1205 06:52:39.542606       1 event.go:294] "Event occurred" object="nsm-system/nsmgr-proxy-f6f56b957" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nsmgr-proxy-f6f56b957-gxbdn"
I1205 06:52:39.601116       1 event.go:294] "Event occurred" object="nsm-system/registry-k8s" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set registry-k8s-7689d6659d to 1"
I1205 06:52:39.618597       1 event.go:294] "Event occurred" object="nsm-system/registry-k8s-7689d6659d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: registry-k8s-7689d6659d-g7hkl"
I1205 06:52:39.662532       1 event.go:294] "Event occurred" object="nsm-system/registry-proxy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set registry-proxy-dd4fdd9fd to 1"
I1205 06:52:39.666654       1 event.go:294] "Event occurred" object="nsm-system/registry-proxy-dd4fdd9fd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: registry-proxy-dd4fdd9fd-d777h"
I1205 06:52:39.760188       1 event.go:294] "Event occurred" object="nsm-system/forwarder-vpp" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: forwarder-vpp-qfmzk"
I1205 06:52:39.788273       1 event.go:294] "Event occurred" object="nsm-system/nsmgr" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nsmgr-lv5n4"
I1205 06:52:39.791078       1 event.go:294] "Event occurred" object="nsm-system/forwarder-vpp" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: forwarder-vpp-fdrhc"
I1205 06:52:39.808541       1 event.go:294] "Event occurred" object="nsm-system/nsmgr" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nsmgr-d8g7g"
I1205 06:53:08.057663       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for networkservices.networkservicemesh.io
I1205 06:53:08.057740       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for networkserviceendpoints.networkservicemesh.io
I1205 06:53:08.057772       1 shared_informer.go:255] Waiting for caches to sync for resource quota
I1205 06:53:08.159733       1 shared_informer.go:262] Caches are synced for resource quota
I1205 06:53:08.321525       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I1205 06:53:08.321987       1 shared_informer.go:262] Caches are synced for garbage collector
I1205 06:53:50.007500       1 event.go:294] "Event occurred" object="kube-system/metrics-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set metrics-server-69dcc44d99 to 1"
I1205 06:53:50.061491       1 event.go:294] "Event occurred" object="kube-system/metrics-server-69dcc44d99" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: metrics-server-69dcc44d99-k6chw"
E1205 06:54:08.147888       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1205 06:54:08.319264       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
I1205 07:11:05.948029       1 event.go:294] "Event occurred" object="istio-system/istiod" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set istiod-5d74c58fdd to 1"
I1205 07:11:06.007142       1 event.go:294] "Event occurred" object="istio-system/istiod-5d74c58fdd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: istiod-5d74c58fdd-9tsmc"
I1205 07:11:08.517685       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for serviceentries.networking.istio.io
I1205 07:11:08.517833       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for wasmplugins.extensions.istio.io
I1205 07:11:08.517847       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for authorizationpolicies.security.istio.io
I1205 07:11:08.517927       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for workloadgroups.networking.istio.io
I1205 07:11:08.517941       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for workloadentries.networking.istio.io
I1205 07:11:08.517956       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for peerauthentications.security.istio.io
I1205 07:11:08.517974       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for istiooperators.install.istio.io
I1205 07:11:08.517984       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for virtualservices.networking.istio.io
I1205 07:11:08.518078       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for telemetries.telemetry.istio.io
I1205 07:11:08.518090       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for proxyconfigs.networking.istio.io
I1205 07:11:08.518098       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for gateways.networking.istio.io
I1205 07:11:08.518107       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for envoyfilters.networking.istio.io
I1205 07:11:08.518129       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for sidecars.networking.istio.io
I1205 07:11:08.518201       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for destinationrules.networking.istio.io
I1205 07:11:08.518241       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for requestauthentications.security.istio.io
I1205 07:11:08.518372       1 shared_informer.go:255] Waiting for caches to sync for resource quota
I1205 07:11:08.809224       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I1205 07:11:09.010295       1 shared_informer.go:262] Caches are synced for garbage collector
I1205 07:11:09.019012       1 shared_informer.go:262] Caches are synced for resource quota
I1205 07:11:21.016854       1 event.go:294] "Event occurred" object="istio-system/istiod" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
E1205 07:11:21.024899       1 horizontal.go:226] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I1205 07:11:21.025125       1 event.go:294] "Event occurred" object="istio-system/istiod" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I1205 07:11:24.165912       1 event.go:294] "Event occurred" object="istio-system/istio-ingressgateway" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set istio-ingressgateway-6795c6c98d to 1"
I1205 07:11:24.213492       1 event.go:294] "Event occurred" object="istio-system/istio-ingressgateway-6795c6c98d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: istio-ingressgateway-6795c6c98d-f92w9"
E1205 07:11:36.143234       1 horizontal.go:226] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I1205 07:11:36.145411       1 event.go:294] "Event occurred" object="istio-system/istiod" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I1205 07:11:36.145436       1 event.go:294] "Event occurred" object="istio-system/istiod" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I1205 07:11:39.193743       1 event.go:294] "Event occurred" object="istio-system/istio-ingressgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
E1205 07:11:39.200375       1 horizontal.go:226] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I1205 07:11:39.200496       1 event.go:294] "Event occurred" object="istio-system/istio-ingressgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
E1205 07:11:54.206840       1 horizontal.go:226] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I1205 07:11:54.207175       1 event.go:294] "Event occurred" object="istio-system/istio-ingressgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I1205 07:11:54.207219       1 event.go:294] "Event occurred" object="istio-system/istio-ingressgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I1205 07:12:09.215769       1 event.go:294] "Event occurred" object="istio-system/istio-ingressgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for any ready pods"
E1205 07:12:09.221548       1 horizontal.go:226] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for any ready pods
I1205 07:12:09.221589       1 event.go:294] "Event occurred" object="istio-system/istio-ingressgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for any ready pods"
I1205 07:12:44.160069       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set istio-eastwestgateway-64b86b4c46 to 1"
I1205 07:12:44.317619       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway-64b86b4c46" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: istio-eastwestgateway-64b86b4c46-cz4dc"
I1205 07:12:59.224501       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for any ready pods"
E1205 07:12:59.227975       1 horizontal.go:226] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-eastwestgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for any ready pods
I1205 07:12:59.228302       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for any ready pods"
I1205 07:16:44.564157       1 event.go:294] "Event occurred" object="sample/helloworld-v1" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set helloworld-v1-78b9f5c87f to 1"
I1205 07:16:44.613466       1 event.go:294] "Event occurred" object="sample/helloworld-v2" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set helloworld-v2-54dddc5567 to 1"
I1205 07:16:44.652208       1 event.go:294] "Event occurred" object="sample/helloworld-v1-78b9f5c87f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helloworld-v1-78b9f5c87f-g5vjn"
I1205 07:16:44.723951       1 event.go:294] "Event occurred" object="sample/helloworld-v2-54dddc5567" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: helloworld-v2-54dddc5567-kv82z"
W1205 07:18:09.979457       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "sample/helloworld", retrying. Error: EndpointSlice informer cache is out of date
I1205 07:58:02.638153       1 event.go:294] "Event occurred" object="ubuntu-ns/ubuntu-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ubuntu-deployment-696fdf7b4 to 1"
I1205 07:58:02.809139       1 event.go:294] "Event occurred" object="ubuntu-ns/ubuntu-deployment-696fdf7b4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ubuntu-deployment-696fdf7b4-qpx6j"
I1205 08:41:32.403783       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set istio-eastwestgateway-586ff4f54f to 1"
I1205 08:41:32.508972       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway-586ff4f54f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: istio-eastwestgateway-586ff4f54f-lrhq4"
E1205 08:41:32.576121       1 disruption.go:614] Error syncing PodDisruptionBudget istio-system/istio-eastwestgateway, requeuing: Operation cannot be fulfilled on poddisruptionbudgets.policy "istio-eastwestgateway": the object has been modified; please apply your changes to the latest version and try again
I1205 08:41:47.607677       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set istio-eastwestgateway-64b86b4c46 to 0 from 1"
I1205 08:41:47.632627       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway-64b86b4c46" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: istio-eastwestgateway-64b86b4c46-cz4dc"
I1205 08:41:48.213423       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for any ready pods"
E1205 08:41:48.219245       1 horizontal.go:226] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-eastwestgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for any ready pods
I1205 08:41:48.219338       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for any ready pods"
E1205 08:42:03.226669       1 horizontal.go:226] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-eastwestgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for any ready pods
I1205 08:42:03.226806       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: did not receive metrics for any ready pods"
I1205 08:42:03.226838       1 event.go:294] "Event occurred" object="istio-system/istio-eastwestgateway" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: did not receive metrics for any ready pods"
I1205 08:49:36.420009       1 cleaner.go:172] Cleaning CSR "csr-g5bnz" as it is more than 1h0m0s old and approved.
I1205 08:49:36.435838       1 cleaner.go:172] Cleaning CSR "csr-kqz5m" as it is more than 1h0m0s old and approved.
==== END logs for container kube-controller-manager of pod kube-system/kube-controller-manager-kind-1-control-plane ====
